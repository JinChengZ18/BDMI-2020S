### Numpy

数学函数库

1. 初始化

Np.array(object, dtype = None, copy = True, order = None, subok = Flase, ndmin = 0)

Np.empty(object, dtype = None, order = None)

Np.ones(object, dtype = None, order = None)

Np.zeros(object, dtype = None, order = None)

2. Broadcast（操作）要求两个矩阵形状相同

3. 形状变换

   reshape（arr, newshape）

   transpose（arr, axes）

4. 排序
5. 线性代数库
6. 载入和保存

### 深度学习1

- **人工神经元**

  带权重的多元函数F（w，x）：输入由x1...xn加权叠加计算出，在经过一个非线性函数（激活函数）得到输出，输出层有可能进行Softmax 操作

  （神经元会计算传送过来的信号的总和，只有当这个总和超过 了某个界限值时，才会输出1。

  这也称为“神经元被激活”。这里将这个界限值称为阈值，用符号θ表示。）

  （超平面：超平面H是从n维空间到n-1维空间的一个映射子空间，分割数据集）

  **激活函数：**

  sigmod函数，逻辑斯提（Logistic）函数
  $$
  sigmod(x) = \frac{1}{1+e^{-x}}
  $$
  tanh/th函数，双曲正切函数
  $$
  tanX(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
  $$
  ReLU(x) = max(x,0)，整流线型单元

  

- **人工神经元模拟布尔电路**

  Logistic Regression Unit 逻辑斯提回归单元

  - 线性运算的单元 AND OR NOT
  - 非线性运算的单元 XOR -> 分解成线性运算的复合

- **人工神经元网络结构**

  1. 前馈网络：输入层、隐藏层、输出层。EX：卷积网络CNNsx，计算快，可并行化

     反馈网络：当前时刻的输出可以作为下一时刻的输入EX：循环网络RNN

  2. 输入是多维数组（即张量Tensor）

     卷积核W：由算法得到的权重参数，数目一般选择16、32、64

     局部区域的权重共享

     卷积层、下采样层、随即丢弃层

  3. 前馈网络万能近似器

  4. 循环网络的近似能力

- **人工神经元网络能力 **:图像识别等...

- **深度学习网络训练**

  如何求W？

  1. 确定目标函数

  2. 可微分网络结构

     反向传播BP，自动微分AD

  3. 通过调整W修正权重：定义损失/成本函数，度量函数D( y , y' )

     - 对于回归任务，通过**均方误差**

     - 对于分类任务，通过**交叉熵**

     损失函数对于权重的梯度计算-反向传播法

     每次沿梯度方向行动一个步长：学习率，一般为0.01；学习率太大会引起震荡，学习率太小收敛太慢

     随机梯度下降法：每次选取随机样本

     batch = min-batch_size * min-batch

     遍历完batch称作一个时代epoch  遍历完min-batch称作一个iteration迭代

### 代码分析

