## 39-Day8

#### 自动微分

误差计算

![image-20200408134317752](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408134317752.png)

ω为参数



### 求导方法

手动微分

数值微分

符号微分

自动微分

##### 数值微分

取极小量，偏置，计算近似切线

容易实现，准确率低

扩展性不够

##### 符号微分

已知一系列基本函数的微分

再定义各种运算的求导法则

##### 自动微分

链式求导法则

把F(x1,x2,...)分解成一堆基本运算

### 

![image-20200408134926072](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408134926072.png)

### Forward Mode

从前往后，一级一级求导数，慢慢往后传

后一级的导数根据前一级的对各个初始自变量的偏导

![image-20200408135232679](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408135232679.png)

先求对x1的偏导

令Δx1=1,Δx2=0

然后一级一级往后传

![image-20200408135410045](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408135410045.png)
$$
\vec Y = \vec F(\vec X)\\ Δ\vec Y = grad (\vec F) Δ\vec X·
$$
缺点：如果输入太多，需要很多次

适用于输入参数较少的情况

### Forward Mode 

### Reverse Mode -后向传播

![image-20200408135926124](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408135926124.png)



vi1表示 y1变化一定量对y1的影响



运行一次，得到的是某个yj对所有输入的偏导

#### 实现-Graph

有时需要储存一些中间量。来计算导数

通过Graph来实现

把反向过程表示成graph

要被用来反向传播的值进行保存，其他值丢弃

![image-20200408140742354](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408140742354.png)

反向传播求导时，建立一个新的graph





## Tensorflow 补充

### 深度学习软硬件布局

tensorflow  谷歌开发

pytorch   facebook开发

软硬件介绍

### 常见深度学习框架

##### tensorflow

深度学习库，python

运行速度较快

gpu，集群，移动端的实现方式

##### pytorch

AI搜索

有一个tensor数据库

有一款IO库

##### berkeley Caffe

C++和python协同编程

有一个非常丰富的预训练库

### 深度学习框架的概念

描述多层网络模型以及训练推断的编程语言及工具类库

编程语言

### tensorflow深度学习框架

计算图

会自动微分，优化网络结构参数

编译器  加速线性代数  优化计算图

### 编译器

![image-20200408142510175](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408142510175.png)

#### 加速线性代数

能够优化tensorflow计算的编译器

#### 对比pytorch的编译器

![image-20200408142614436](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408142614436.png)

#### TVM编译器



### tensorflow名称来历

利用数据流图进行数值计算的软件库

图中的节点表述数学运算

图中的边表示他们传递的数据（张量）

张量从图的一端流动到另一端，即tensorflow

tensor的属性：rank（维数），shape,datatype

#### 立即执行 

tensorflow2.0实现

定义了一个函数f(g(x+y),2*x)

在声明这个函数的时候，就立即计算x+y等

C属于立即执行

不立即执行：声明了c=a+b的时候，c并没有没计算，只是记录了这么一个函数关系

### Keras

一套高级API，用来快速搭建神经网络

基于Tensorflow实现的程序库

### 计算图CG

基于人工神经元的代码分析

### 求导

采用符号微分方法

由后往前，从最后一个节点开始，找到所有依赖C的节点I，并计算倒数

### 底层

C++的Eigen库

gemmlowp 低精度矩阵库的加快量化运算

### 数据流图

包括训练数据的读取、转换、队列

县独去数据，得到一个混洗队列

在经过预处理，得到一个有序的队列

读取参数，前向传播，计算结果

根据损失函数，反向更新梯度，更新网络模型参数

同时，每一段周期进行一次检验，查看训练效果，判断是否停止



### 程序

选择模型

##### 构建网络层

输入层、隐藏层，输出层

选择的初始化方法，激活函数、损失函数

##### 编译

优化函数。孙是函数，幸能评估

##### 训练

回调函数，数据预处理

##### 预测

### 架构

上层是训练库和腿短裤，步数最终生成的模型在不同设备上

中间层是python和C++

底层，网络，设备



### 编程模型

层级，数据集

![image-20200408144810181](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408144810181.png)

##  卷机网络（convolution network)

#### 卷积运算

一种张量运算

输入是多维数组的数据

卷积核是一个多维数组

#### 卷积核

通道数和输入相同

##### 参数

输入长度W

卷积核大小F

移动的步长S

填充  zero padding

输出长度 L = (W-F+2P)/S+1

##### 并行化

各个卷积核可以并行运行

### 卷积网络

加入了权值共享，池化，补零等操作

### 发展

#### 新的神经网络不断推出

![image-20200408150135581](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408150135581.png)

#### 网络的深度和规模都在提升，准确率提升

1*1卷积层，全局池化

VGG 降参

残差思想  增加了skip connection，跨层传播

### 深度卷积网络--人脸识别

一层层提取特征

从局部到整体

## 循环网络 Recurrent network

#### RNN

在时间维度上，内一个时间不处理时，共享相同的权重

用于模型检测预测问题

![image-20200408150827509](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408150827509.png)

#### 网络结构

y 训练目标

L 孙是函数

o 网络输出

h 状态/隐藏单元

x 输入

![image-20200408151027802](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408151027802.png)



#### 权值共享？

在不同的时间不上采用相同的UVW参数



### 双向RNN？

一个时间上从序列起点开始移动的RNN，和另一个时间上从序列末尾开始移动的RNN





### 通过时间反向传播？



### RNN训练的问题

梯度消失、梯度爆炸问题

最初和最后的时间步的梯度过大或者过小

梯度截断：当梯度大于一定值时。设为最大值

### 长短时记忆网络LSTM

记忆单元  输入门   遗忘门  输出门

增加了一个主输入单元和其他辅助门

辅助单元可以寄存时间序列的输入，在训练过程中会利用后向传播的方式进行。

记忆单元和这些门单元的组合，大大提升了RNN处理远距离依赖问题的能力 ，解决RNN网络收敛慢的问题

#### 前向方程

![image-20200408151855752](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408151855752.png)

### 门控循环单元

对LSTM的建华

把LSTM的输入们和遗忘门合并，用于控制历史信息



### 序列对序列模型

输入对输出问题



## CNN的实现

### 深度网络

依照拓扑连接结构，将大量的神经元组织起来，构成规模化的深度的网络

拓扑结构：每一层

### 多层网络

全连接网络Dense

#### Dence与CNN

#### RNN与LSTM



###  Neruron ->layer

单个人工神经元，输入ld,参数ld，输出标量0d

一层人工神经元构成一个Layer

输出的shape和Layer的shape一致

batch_size 会影响输出的shape

超参数U 神经元的个数

###### softmax层

归一化，计算概率分布向量

##### 多层感知机

由多个Dense层组成

没有激活函数

可以通过softmax计算一个概率分布

##### 常用于图像分类



#### CNN

全连接对应的权重网络较大

局部权重共享

卷积层后面通常跟着一个下采样层

##### 卷积层

用于处理图像

超参数：卷积核个数，核大小，补零数，步长

![image-20200408154847262](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408154847262.png)

##### 池化层

缩小模型规模

结构是3d的

超参数

最大池化，平均池化

##### Dropout层

减少CNN的过拟合

超参数：丢弃率

对于

![image-20200408155212558](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408155212558.png)

对于所有的输入, 有keep_prob概率保

留并乘以1/keep_prob, 以保证前后总

和大致相等, 否则输出0



### RNN

##### 概念

相当于一个三层全连接

h有一个环，意味着这个时候的参数会影响下一个时候的参数

权重共享：

##### 计算图

通过上一时刻的状态,输入参数,b的叠加得到a

a激活得到h

##### 输入和loss处理

给定输入长度，把输入序列化

输入离散化

收集所有时刻的输出，计算loss

损失：负对数似然

##### RNN实例

![image-20200408160030992](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408160030992.png)

h输入，通过U矩阵得到隐藏层，再输出，期望为e

然后e输入，通过U矩阵和前一级的隐藏层得到这时候的隐藏层，再通过隐藏层输出

#### 实现

keras.layers.RNN



### LSTM

解决梯度爆炸和梯度小时的问题

门单元：Gate  

控制数据通过Sigmoid函数后，变成了一个范围在0-1之间的tensor

增加了一个辅助记忆单元和三个辅助门单元

控制是否输入、存储、输出

增加了远距离处理能力

![image-20200408160802949](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408160802949.png)



通过ht-1和xt得到这时候的控制信号ft

ft决定要遗忘上一级Ct的多少比例



#### 实现

![image-20200408160932277](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408160932277.png)

unit,激活函数，

#### GRU layer

把遗忘门和输入们组合为一个单一的更新们

如果重置门关闭，会忽略掉历史信息

LSTM的cell状态

![image-20200408161112697](C:\Users\huawei\AppData\Roaming\Typora\typora-user-images\image-20200408161112697.png)

