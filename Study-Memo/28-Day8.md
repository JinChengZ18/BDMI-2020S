# Day8 学习小结  

## 1.自动微分
### background  
 为什么要使用自动微分：得到损失函数后通过求导更新参数  
1. manual differentation(手动微分)  
就是手动算微分  
2. numerical differentiation（数值微分）  
用偏导数进行数值近似，但不太精确  
3. symbolic differentiation（符号微分）  
通过一些求导法则；可能造成表达爆炸；而且通用性不够广  
4. automatic differentiation  
### forward mode  
1. 根据graph，对于由$x_1、x_2$求出y，根据链式法则通过中间的$v_1、v_2$等等依次求出偏导，最后得到y对于某一个输入的偏导。  
2. 需要对多个输出偏导时，需要进行多次运算  
### forward mode：dual value  
f(a+bϵ) = f(a)+h'(a)*ϵ  
### 反向自动微分  
先用forward mode计算图中每个结点的值，再反向计算所有的导数取值  
&nbsp;

## 2.Tensorflow  
### 深度学习框架  
+ 编程语言（tensorflow会自动微分），解释器（对应计算图），编译器（XLA）  
### tensorflow介绍  
+ 以运算为图的节点，边是张量（传输的多维矩阵）  
+ keras可以用于快速搭建神经网络  
+ 求导采用符号微分  
### tensorflow编程模型
&nbsp;

## 3.卷积网络  
1. 卷积核是一个多维数组，其中参数由学习算法得到   
2. 输出长度计算：  
+ 输入的长度  
+ 卷积核大小  
and so on  
3. 可以用于图像识别等等  
&nbsp;
## 4.循环网络-RNN
+ 在时间维度上，每一个时间步处理，采用共享的权重  
+ 适用于时间序列输入，例如对联生成、诗歌、天气预报等等  
+ 可能有梯度爆炸  
### 长短时记忆网络（LSTM）  
+ 相比RNN增加了记忆单元、输入们、遗忘门（选择性遗忘记忆单元的内容）、输出门  
+ 对历史信息进行一次筛选(×) + 输入新信息（+）   
+ 改进：GRU（门控循环单元）  
&nbsp;
## 5.多层网络  
###  学习路线：  
+ dense/cnn  
+ RNN LSTM  
### neuron layer  
1. 单个人工神经元：  
+ 输入1d，参数1d，输出0d(标量)  
2. 一层人工神经元构成一个layer  
3. dense layer 结构是1d  
### CNN  
1. 卷积网络特点：局部区域权重W共用  
2. 每一个卷积层后紧跟着一个下采样层，如最大池化  
3. pooling层 意义：采样，缩小模型大小  
### LSTM  
+ 门单元Get 经过sigmoid函数之后变成一个在0-1的之间的tensor  
+ 辅助记忆单元可以寄存时间序列，在训练过程中用后向传播的方式进行  
+ 遗忘门和输入门合并成更新门；重置门关闭会忽略掉历史信息  


