# Day8 学习小结  

## 自动微分
读取参数，前向计算中间值，进行预测
反向微分，更改参数，优化模型
为了后向过程更快更省空间，前向过程中合适的量被特意存储
## TensorFlow
求导计算图：表达数据、参数之间的计算关系
数据流图：表达模型的优化过程
### 运行过程：
建立模型
构建网络
编译
训练：前向、后向
预测
### tensorflow架构：
上层为训练库推断库
中层为c++与python的借口
底层为硬件操作
## 卷积网络与循环网络
### 前馈的卷积网络
### 反馈的循环网络
### lstm 
对C使用一个乘法器和加法器实现筛选历史信息并叠加处理后的当前信息，是lstm中的关键设计环节
对h，h是C和x的融合，关系到预测的输出，加入x更好预测
### tensor
dimension、length、shape、volume
### 深度网络
又拓扑结构，划分为前馈网络、反馈网络、和记忆网络
### 多层网络
多层全连接网络（Dense密集）
### Dense网络
按batch输出时，输入数据变成多维（二维，一列表示一组样本）
排列结构：layer全1d
超参数：神经元个数为U
输入长度为L
参数共L*U个
最后对U个输出进行Softmax处理
#### 图像
矩阵表示像素集合，每个元素表示一个像素：
一维二值表示、RGB三维表示
对彩色图片，200*200*3是单个神经元需要的参数，太多了，需要卷积
#### 卷积计算
是张量运算，输入是多维数组，卷积核数目一般选16、32、64是由学习算法得到的权重参数
CNN包括卷积层、采样层、正则层
卷积层用于处理输入数据，降低输入的维度
采样层用于减小模型大小，减小输出
正则层进行drop out，抛弃不必要的参数甚至神经元
### 循环网络
对于隐藏单元的参数h，有循环优化的训练过程
UVW分别是输入到隐藏，隐藏到隐藏，隐藏到输出
#### 简单的计算方法：
输入的线性组合的非线性激发再线性组合再softmax，得到输出
#### 输入和loss的处理
输入离散化且序列化（one-hot处理种类类型的输入）
按照损失函数计算导数用于循环修正参数
### lstm
处理RNN梯度爆炸和梯度消失的问题
主要的特点是有记忆性
#### 门单元Gate
作为开关筛选输入的每个元素
输入门、遗忘门、输出门
#### GRU
遗忘门与输入门组合构成更新门，新增重置门


