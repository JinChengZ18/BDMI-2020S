# Numpy
.ndim ：维度
.shape ：各维度的尺度 （2，5）
.size ：元素的个数 10
.dtype ：元素的类型 dtype(‘int32’)
.itemsize ：每个元素的大小，以字节为单位 ，每个元素占4个字节
ndarray数组的创建
np.arange(n) ; 元素从0到n-1的ndarray类型
np.ones(shape): 生成全1
np.zeros((shape)， ddtype = np.int32) ： 生成int32型的全0
np.full(shape, val): 生成全为val
np.eye(n) : 生成单位矩阵

np.ones_like(a) : 按数组a的形状生成全1的数组
np.zeros_like(a): 同理
np.full_like (a, val) : 同理

np.linspace（1,10,4）： 根据起止数据等间距地生成数组
np.linspace（1,10,4, endpoint = False）：endpoint 表示10是否作为生成的元素
np.concatenate():
numpy.squeeze

载入和保存
# 深度学习
## 人工神经元
激活函数

输入由x1...xn加权叠加计算出，再经过一个非线性函数（带权重的多元函数F（w，x））,

得到输出，输出层有可能进行Softmax 操作，神经元被激活

神经元会计算传送过来的信号的总和，只有当这个总和超过 了某个界限值时，才会输出1，这里将这个界限值称为阈值，用符号θ表示。

## 模拟布尔电路

sigmod函数，Logistic函数

$$ sigmod(x) = \frac{1}{1+e^{-x}} $$

tanh/th函数，

双曲正切函数

$$ tanX(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}} $$

ReLU(x) = max(x,0)，整流线型单元

Logistic Regression Unit 逻辑斯提回归单元

前馈网络：输入层、隐藏层、输出层。EX：卷积网络CNNsx，计算快，可并行化

反馈网络：当前时刻的输出可以作为下一时刻的输入EX：循环网络RNN

输入是多维数组（即张量Tensor）

局部区域的权重共享

卷积层、下采样层、随即丢弃层

前馈网络万能近似器，循环网络的近似能力

#深度学习网络训练

如何求W？通过不断地向前和向后微分调整参数得到最优解

确定目标函数，可微分网络结构，反向传播BP，自动微分AD，通过调整W修正权重：定义损失/成本函数，度量函数D( y , y' )

对于回归任务，通过均方误差
对于分类任务，通过交叉熵
损失函数对于权重的梯度计算-反向传播法

每次沿梯度方向行动一个指定步长，步长可为超参数后期进行调整，学习率一般为0.01；学习率太大会引起震荡，导致难以收敛到目标点，学习率太小收敛太慢，耗费计算资源则较多

随机梯度下降法：每次选取随机样本
