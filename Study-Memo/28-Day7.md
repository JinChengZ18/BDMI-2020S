# Day7 学习小结  

## Numpy  
1. 运行速度非常快的数学库，主要用于数组计算  
2. 特点：N维数组对象 ndarray(必须要同类型数据)  
3. numpy.zeros(shape, dtype = float, order = 'C')  
    创建指定大小的数组，元素以0填充  
    order: C按行，F按列，A原顺序，K内存中的出现顺序  
4. numpy.empty(...)  元素为空  
5. numpy.ones(...) 元素以1填充  
6. numpy.reshape(arr, newshape, order = 'C')   
7. numpy.ravel  
8. numpy.transpose  
9. numpy.concatenate((a1,a2,a3...), axls) 沿指定轴连接相同形状的多个数组  
10. numpy.hstack 垂直堆叠  
    
    ## 人工神经元  
1. 人工神经元：带权重的函数  
2. 激活函数(神经元中非线性变化函数)   
    sigmoid函数，tanh函数，ReLU函数  
3. 人工神经元实现布尔电路(逻辑斯提回归单元)  
+ 利用偏置系数$w_i$实现  
+ 用NAND神经元（NAND）实现复杂的布尔电路  
    与非门可以表示所有门    
+ 用人工神经元判断性别：对不同输入特征赋以权重（确定权重：经验调参法）  
4. 人工神经网络  
+ 前馈网络  
+ 卷积网络CNN：  
  局部区域的权重：每一个卷积层后紧跟着下采样层；局部区域的权重共享  
+ 循环网络（不同时间步采用相通的U V W权重矩阵；即训练完成之前权重矩阵不 会改变）  
5. 万能近似定理  
 无论试图学习什么函数，存在一大的MLP一定能够表示这个函数（但实际上不一定能够学到这个函数）  


## 激活函数示例  
1. ReLU(x) = max(0,x)  
可用于整流  
2. PReLU（x）  
3. sigmoid函数  $sigmoid(x) = 1/1+e^{-x}$  
1. 饱和型S函数   
2. logit函数 log(x/(1-x))  
3. softmax 总和为1，可以看成概率   

## 深度学习  
### 深度神经网络训练  
+ 确定目标函数（考虑 loss)  
+ 可微分网络结构  
    反向传播 自动微分  
+ 修正权重  
+ 采用带标签的样本进行学习，确定权重  
+ 度量函数D(y, y') 衡量y与y'的差异，努力使其差异值最小  
+ 交叉熵:用于分类任务  
+ 回归任务用均方误差计算损失，分类任务用交叉熵  
+ 梯度下降法：找到局部极小值，向当前点对应梯度的反方向迭代搜索  
+ 随机梯度下降法：调整权重最小化损失函数  
+ 步长/学习率：一般为0.01，太小收敛太慢，太大会震荡  
+ 数值计算中要保证数值稳定（舍入误差不增长）  
+ page：固定大小的一些储存单元  
+ file：variable list of page  
+ read：disk $\rightarrow$buffer  
+ flush:evict page  from buffer &write to disk,如果文件修改  
### tensorflow实现  
+ tensor：多维数组  
+ 张良表示： X H N Y W1 W2都是张量  


