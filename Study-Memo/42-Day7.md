





# 第七次课小结

------

[TOC]

 **主要内容：Numpy，人工神经元，神经网络原理，tensorflow实现**



## Numpy

1. 运行速度非常快的数学库，主要用于数组计算
2. 特点：N维数组对象 ndarray(必须要同类型数据)
3. numpy.zeros(shape, dtype = float, order = None)
   创建指定大小的数组，元素以0填充
   **order: C按行，F按列，A原顺序，K内存中的出现顺序**
4. numpy.empty(shape, dtype = float, order = None)  元素为空
5. numpy.ones(shape, dtype = float, order = None) 元素以1填充
6. numpy.reshape(arr, newshape, shape, dtype = float, order = None)
7. numpy.ravel：将多维数组按次序转化为一维数组
8. numpy.transpose(arr)：转置
9. numpy.concatenate((a1,a2,a3...), axls) 沿指定轴连接相同形状的多个数组
10. numpy.vstack 垂直堆叠(表示轴0合并)
11. numpy.hstack 水平合并(表示轴1合并)





## 人工神经元

#### 1.**人工神经元**：带权重的函数

#### 2.激活函数

 **(神经元中非线性变化函数)**

sigmoid函数：$$ sigmod(x) = \frac{1}{1+e^{-x}} $$

tanh函数

ReLU函数：$max(x,0)$                   可用于整流

双曲正切函数：$$ tanX(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}} $$

PReLU(带参数的ReLU)：$PReLu(x_i)=\begin{cases} x_i，if x_i\geq0\\ a_ix_i， if x_i\leq0\end{cases}  $

logit函数： $ln(\frac{x}{1-x})$

饱和S函数：$sat(x_i)=\begin{cases} 1，if:x_i>\Delta\\ kx_i， if:|x_i|\leq\Delta\\-1,if:x_i<-\Delta\end{cases}  $

#### 3.模拟布尔电路

- 利用偏置系数$$w_i$$实现
- 利用ReLu函数完美地线性划分异或问题
- 用NAND神经元（NAND）实现复杂的布尔电路
- 与非门可以表示所有门
- 用人工神经元判断性别：对不同输入特征赋以权重（确定权重：经验调参法）

#### 4.**人工神经网络**

- 前馈网络：输入层、隐藏层、输出层。

- 卷积网络CNN：卷积层(卷积核W)，采样层(池化层)，丢弃层
  局部区域的权重：每一个卷积层后紧跟着下采样层；局部区域的权重共享

- 循环网络RNN（在时间上共享U V W权重矩阵，训练完成之前权重矩阵不会改变

  ​	主要是为了刻画**一个序列当前的输出与之前信息的关系**

#### 5.**万能近似定理**

无论试图学习什么函数，存在一大的前馈网络一定能够表示这个函数（但实际上不一定能够学到这个函数，因为没有规定收敛速度）

循环网络的近似能力：RNN是图灵机



## 深度学习

- https://playground.tensorflow.org/

### 深度神经网络训练

##### **建立网络**

- 确定目标函数
- 建立可微分网络结构
  反向传播$\rightarrow$ 自动微分
- 初始化权重和偏置矩阵
- 采用带标签的样本进行学习，修正权重和偏置

##### **损失函数**

- 损失函数D(y, y') 衡量y与y'的差异，努力使函数值最小
- **回归任务用均方误差计算损失，分类任务用交叉熵**

##### **优化方法**

- 梯度下降法：找到局部极小值，向当前点对应梯度的反方向迭代搜索

  ​	沿着loss function下降最快的方向更新参数

- 随机梯度下降法：调整权重最小化损失函数

- 步长/学习率：一般为0.01，太小收敛太慢，太大会震荡

- 数值计算中要保证数值稳定（舍入误差不增长）

##### **实际训练**

- batch training，每次送几个训练样本进去训练，损失更新时应该用微积分求导法则。

### tensorflow实现

- 操作对象：tensor(多维数组)

- tensor与numpy arrary无缝衔接

  
