# 课程小结

- 自动微分，前向传播和后向传播
- 深度学习框架和TensorFlow
- 卷积神经网络，循环神经网络

## Automatic Differentiation

- Manual differentiation（手动微分）
- Numerical differentiation（数值微分）
  - 利用微分定义，用偏导数进行数值近似
  - 精度不高
- Symbolic differentiation（符号微分）
  - 利用基本函数导数和导数的性质
  - 可能造成表达式爆炸
- Automatic differentiation（自动微分）
  - 基于计算图和链式法则，对图中的每个环节分别求导，再组合起来
  - 模式
    - 前向传播（Forward mode）：从自变量向后逐步计算微分，计算每个节点∂v<sub>i</sub>/∂x（其中v<sub>i</sub>是节点，x是输入），得到每个输出对某一输入的偏导
    - 后向传播（Reverse mode）：从结果向前逐步计算微分，计算每个节点∂y/∂v<sub>i</sub>（其中v<sub>i</sub>是节点，y是输出），得到某一输出对每个输入的偏导

## TensorFlow

- DeepLearning
  - 编程语言，解释器，编译器
  - 常见框架：TensorFlow, pyTorch, Caffe
- TensorFlow
  - 以运算为图的节点，边是张量
  - 求导采用符号微分

## CNN和RNN

- 全连接层Dense
  - 超参数：层数
  - input: L, weight: L*U, output: U
- 多层感知机MLP
  - 多个全连接层，用于解决分类问题
  - 最后输出概率向量需通过Softmax层

- 分类
  - 前馈网络：MLP, CNN
  - 反馈网络：RNN
  - 记忆网络
    - 长短时记忆网络LSTM
      - 比RNN增加了记忆单元，解决RNN远距离收敛慢的问题
      - 输入门input gate、遗忘门forget gate（选择性遗忘记忆单元的内容）、输出门output gate
      - 对历史信息进行一次筛选（×） + 输入新信息（+）
      - 辅助记忆单元寄存时间序列，训练过程中后向传播
    - 门控循环单元GRU
      - LSTM的简化和改进
      - 遗忘门和输入门合并成更新门update gate，重置门reset gate控制历史信息的传入

- 卷积神经网络CNN
  - 卷积核在输入数据上滑动加权求和，可以减少参数量
  - 局部区域权重共用

- 循环神经网络RNN
  - 在时间维度，在不同的时间步上采用相同的权重矩阵
  - 问题：梯度消失或梯度爆炸；训练时常需要进行梯度截断以避免梯度爆炸

- 用TensorFlow和keras实现几种基本的神经网络架构
