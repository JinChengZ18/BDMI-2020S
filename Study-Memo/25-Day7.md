## Numpy

### 初始化,建立数组 np.array（）



Np.array(object, dtype = None, copy = True, order = None, subok = Flase, ndmin = 0)



Np.empty(object, dtype = None, order = None)



Np.ones(object, dtype = None, order = None)



Np.zeros(object, dtype = None, order = None)



### Broadcast（操作）

要求两个矩阵形状相同

### 形状变换

reshape（arr, newshape）

transpose（arr, axes）

### 排序

### 线性代数库

numpy.squeeze

### 载入和保存



### 深度学习

#### 人工神经元

**激活函数**

输入由x1...xn加权叠加计算出，再经过一个非线性函数（带权重的多元函数F（w，x））, 

得到输出，输出层有可能进行Softmax 操作



**神经元被激活**

神经元会计算传送过来的信号的总和，只有当这个总和超过 了某个界限值时，才会输出1。

这里将这个界限值称为阈值，用符号θ表示。



**模拟布尔电路**

sigmod函数，Logistic函数

 $$ sigmod(x) = \frac{1}{1+e^{-x}} $$ 

tanh/th函数，

双曲正切函数

 $$ tanX(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}} $$ 

ReLU(x) = max(x,0)，整流线型单元



**人工神经元模拟布尔电路**

Logistic Regression Unit 逻辑斯提回归单元

- 线性运算的单元 AND OR NOT
- 非线性运算的单元 XOR -> 分解成线性运算的复合



**人工神经元网络结构**

1. **前馈网络**：输入层、隐藏层、输出层。EX：卷积网络CNNsx，计算快，可并行化

   **反馈网络**：当前时刻的输出可以作为下一时刻的输入EX：循环网络RNN

2. 输入是多维数组（即张量Tensor）

   卷积核W：由算法得到的权重参数，数目一般选择16、32、64

   局部区域的权重共享

   卷积层、下采样层、随即丢弃层

3. 前馈网络万能近似器

4. 循环网络的近似能力



**人工神经元网络能力 **

图像表示——每个像素是数字

手写数字MNIST：

灰度图像（8位2进制），二值图像

时尚MNIST：

24位2进制，RGB

示例：softmax



**深度学习网络训练**

如何求W？

1. 确定目标函数

2. 可微分网络结构

   反向传播BP，自动微分AD

3. 通过调整W修正权重：定义损失/成本函数，度量函数D( y , y' )

   - 对于回归任务，通过**均方误差**
   - 对于分类任务，通过**交叉熵**

   损失函数对于权重的梯度计算-反向传播法

   每次沿梯度方向行动一个步长：

   学习率，一般为0.01；学习率太大会引起震荡，学习率太小收敛太慢

   随机梯度下降法：每次选取随机样本

   batch = min-batch_size * min-batch

   遍历完batch称作一个时代epoch  遍历完min-batch称作一个iteration迭代

   page：固定大小的一些储存单元；file：variable list of page

   read：disk $\rightarrow$buffer



**tensorflow**