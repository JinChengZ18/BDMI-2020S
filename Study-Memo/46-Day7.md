# 深度学习

深度学习是一种针对一个特定问题，进行长时间预处理的算法，算法正确性并不保证，但是正确率一般很高。

深度学习能解决的问题一般上都是需要经验结合主观来解决的，传统算法几乎不能解决该类问题。

深度学习的思路就是建立一套神经系统，根据输入的数字，将这些数字经过精密调试过的逻辑拓扑结构，给出不同答案的百分比，最后选择最高百分比的答案作为输出。

逻辑拓扑结构就是“神经网络”，一般见到的是分层，一层的结点只与下一层的结点有连接关系，成为前馈网络。

深度学习需要一个数据集进行“学习”，“学习”指的就是调整数据传递时用到的权重(weight)和偏移量(bias)。

以下为几个主要的深度学习概念

1. Forward propagation 前向传递
2. Backward propagation 反向传递
3. Gradient Descend 
4. Error function
5. Automatic Differentiation 自动微分
6. Activation function 激活函数

前向传递就是计算每个结点输出后然后到下一层去继续计算。这些值在后向传递时会用到。

后向传递就是根据error 函数（交叉熵），然后从最后一层开始往回退，更新权重和bias。更新的时候会用到自动微分和gradient descend的思想。

Gradient descend需要结合多元微积分的知识。在二元微积分我们知道(x,y)对应f(x,y)形成一个三维空间。grad f = (偏f偏x,偏f偏y)，是一个xy平面上的向量，向量指向f的极大值。所以如果我们求该向量的负值，就得到了指向最小值的向量，也就是我们在最小化f。（注意，这里最小化不是全局最小）

由于每次从输入前馈到输出后会和一个理想答案比较，得到一个值，而这个值和每个权重的关系能通过偏error函数偏weight_i求得。<新权重i = 旧权重i - 学习率 * (偏error偏权重 i)>

自动微分是一种求导的方式。这个方式在下次笔记会提到。

每个神经元都是由前层的神经元的输出加权求和后经过一个激活函数，激活函数是将输出固定到一个区间去。一些常见的例子包括：sigmoid函数，ReLU函数



