# WEEK8 学习小结

程宇笑 自84 2018010888

---

## AD 自动微分

深度学习的实现离不开微分运算，手动微分更多依赖于表达式的求解，而自动微分则偏向微分的定义——极小量的求解。

$\frac{\partial f(\mathbf{x})}{\partial x_{i}} \approx \frac{f(\mathbf{x}+h\mathbf{e_{i}})}{h}, h \rightarrow 0$

然而这种数值方法存在精度不够的问题，同时符号微分的方法又适用性不广。

### 前向传播算法

通过链式法则，可以计算输出对输入作为自变量求偏导的结果。这种方法比较**适用于输入较少、输出较多**的情况。实际实现过程中，通常对求导过程的非线性项进行**线性近似**，即

$f(v+\dot{v}) = f(v)+f'(v)\dot{v}$

可以总结说，所谓的前向传播算法就是：**将上一层的输出作为下一层的输入**，并计算下一层的输出，一直到运算到输出层为止。问题在于如果要求输出对**某一个**输入的导数，相当于需要累加“求导路径”上所有结果，因此不同导数的运算用到了很多重复结果。总体而言计算会比较繁杂、误差也会比较大。

### 反向传播算法

反向传播算法使用“从后往前”的算法，其优越之处在于**避开了重复计算的冗余**，它对于每一个路径只访问一次就能求顶点对所有下层节点的偏导值。

具体方法是，首先使用前向传播方法计算出每个右层对左层的导数，然后从最右层向左逐层“发送”偏导值，从而求出最右层对输入的导数。


```python
import tensorflow as tf

x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0

with tf.GradientTape() as t:
  with tf.GradientTape() as t2:
    y = x * x * x
  # Compute the gradient inside the 't' context manager
  # which means the gradient computation is differentiable as well.
  dy_dx = t2.gradient(y, x)
d2y_dx2 = t.gradient(dy_dx, x)
print(dy_dx)
print(d2y_dx2)
assert dy_dx.numpy() == 3.0
assert d2y_dx2.numpy() == 6.0
```

    tf.Tensor(3.0, shape=(), dtype=float32)
    tf.Tensor(6.0, shape=(), dtype=float32)
    

## Tensorflow 框架

Tensorflow 求导采用符号微分方法，借助“**梯度计算图**（CG）”完成梯度下降算法。

Tensorflow 的数据流基本模式是：输入 - 预处理 - （训练 - 网络模型参数更新）循环进行 - 周期检验 - 分布式文件系统

### 卷积神经网络（CNN）

**卷积**（Convolution）是一种张量运算，实际上是将卷积核和输入数据对应部分相乘累加。对二维数据进行卷积的结果比输入更**小**，因此需要填充。

卷积网络包括**卷积层**（卷积核是多维数组，在学习中改变），**下采样层（池化层）**（进行最大值或最小值的抽样，输入下一层），**随机丢弃层**（每一次学习中，禁用随机的一些神经元，这对于防止过拟合很有用）。

### 循环神经网络（RNN）

循环神经网络可将上一时刻的输出作为当前时刻的输入，并进行参数共享。非常适合语音、文字等信息的处理。

RNN 容易出现梯度消失和梯度爆炸的问题（梯度过大或过小），应该限制为一些特定数值，或者使用LSTM。

### 长短时记忆网络（LSTM）

LSTM 和 RNN 的区别在于，LSTM 拥有记忆单元、遗忘门，相比普通的RNN，LSTM能够在更长的序列中有更好的表现。

LSTM 内部主要包含忘记阶段、选择记忆阶段、输出阶段。

关于 RNN 和 LSTM 的更多知识可见[史上最详细循环神经网络讲解（RNN/LSTM/GRU）](https://zhuanlan.zhihu.com/p/123211148)
